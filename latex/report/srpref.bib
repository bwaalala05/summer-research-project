@article{OBrien2007,
	abstract = {The Variance Inflation Factor (VIF) and tolerance are both widely used measures of the degree of multi-collinearity of the ith independent variable with the other independent variables in a regression model. Unfortunately, several rules of thumb - most commonly the rule of 10 - associated with VIF are regarded by many practitioners as a sign of severe or serious multi-collinearity (this rule appears in both scholarly articles and advanced statistical textbooks). When VIF reaches these threshold values researchers often attempt to reduce the collinearity by eliminating one or more variables from their analysis; using Ridge Regression to analyze their data; or combining two or more independent variables into a single index. These techniques for curing problems associated with multi-collinearity can create problems more serious than those they solve. Because of this, we examine these rules of thumb and find that threshold values of the VIF (and tolerance) need to be evaluated in the context of several other factors that influence the variance of regression coefficients. Values of the VIF of 10, 20, 40, or even higher do not, by themselves, discount the results of regression analyses, call for the elimination of one or more independent variables from the analysis, suggest the use of ridge regression, or require combining of independent variable into a single index. © 2007 Springer.},
	author = {Robert M. O'Brien},
	doi = {10.1007/s11135-006-9018-6},
	issn = {00335177},
	issue = {5},
	journal = {Quality and Quantity},
	keywords = {Multi-collinearity,Tolerance,Variance inflation factors,Variance of regression coefficients},
	month = {10},
	pages = {673-690},
	title = {A caution regarding rules of thumb for variance inflation factors},
	volume = {41},
	year = {2007}
}
@article{Johnston2018,
	abstract = {Many ecological- and individual-level analyses of voting behaviour use multiple regressions with a considerable number of independent variables but few discussions of their results pay any attention to the potential impact of inter-relationships among those independent variables—do they confound the regression parameters and hence their interpretation? Three empirical examples are deployed to address that question, with results which suggest considerable problems. Inter-relationships between variables, even if not approaching high collinearity, can have a substantial impact on regression model results and how they are interpreted in the light of prior expectations. Confounded relationships could be the norm and interpretations open to doubt, unless considerable care is applied in the analyses and an extended principal components method for doing that is introduced and exemplified.},
	author = {Ron Johnston and Kelvyn Jones and David Manley},
	doi = {10.1007/s11135-017-0584-6},
	issn = {15737845},
	issue = {4},
	journal = {Quality and Quantity},
	keywords = {Collinearity,Confounding,Regression analysis,Voting behaviour},
	month = {7},
	pages = {1957-1976},
	publisher = {Springer Netherlands},
	title = {Confounding and collinearity in regression analysis: a cautionary tale and an alternative procedure, illustrated by studies of British voting behaviour},
	volume = {52},
	year = {2018}
}
@article{Shabuz2024,
	abstract = {The cos-max method is a little-known method of identifying collinearities. It is based on the cos-max transformation, which makes minimal adjustment to a set of vectors to create orthogonal components with a one-to-one correspondence between the original vectors and the components. The aim of the transformation is that each vector should be close to the orthogonal component with which it is paired. Vectors involved in a collinearity must be adjusted substantially in order to create orthogonal components, while other vectors will typically be adjusted far less. The cos-max method uses the size of adjustments to identify collinearities. It gives a coherent relationship between collinear sets of variables and variance inflation factors (VIFs) and identifies collinear sets using more information than traditional methods. In this paper we describe these features of the method and examine its performance in examples, comparing it with alternative methods. In each example, the collinearities identified by the cos-max method only contained variables with high VIFs and contained all variables with high VIFs. The collinearities identified by other methods did not have such a close link to VIFs. Also, the collinearities identified by the cos-max method were as simple as or simpler than those given by other methods, with less overlap between collinearities in the variables that they contained.},
	author = {Zillur R. Shabuz and Paul H. Garthwaite},
	doi = {10.1111/anzs.12425},
	issn = {1467842X},
	issue = {3},
	journal = {Australian and New Zealand Journal of Statistics},
	keywords = {auxiliary regression,cos-max,eigenvector analysis,variance decomposition,variance inflation factor},
	month = {9},
	pages = {367-388},
	publisher = {John Wiley and Sons Inc},
	title = {Examining collinearities},
	volume = {66},
	year = {2024}
}
@book{McClave2018,
	abstract = {},
	author = {James McClave and Terry Sincich},
	edition = {13},
	isbn = {978-1-292-16155-6},
	keywords = {21, 89,22, 89,23, 89},
	publisher = {Pearson Education Limited},
	title = {Statistics},
	year = {2018}
}
@book{Agresti2018,
	author = {Alan Agresti and Christine Franklin and Bernhard Klingenberg},
	isbn = {9780321997838},
	publisher = {Pearson Education Limited},
	title = {Statistics : the art and science of learning from data},
	year = {2018}
}

@book{James2023,
	author = {Gareth James and Daniela Witten and Trevor Hastie and Robert Tibshirani and Jonathan Taylor},
	month = {7},
	publisher = {Springer},
	title = {Introduction to Statistical Learning with Applications in Python},
	year = {2023}
}

@book{Agresti2015,
	author = {Alan Agresti},
	isbn = {978-1-118-73003-4},
	publisher = {John Wiley and Sons Inc.},
	title = {Foundations of Linear and Generalized Linear Models},
	year = {2015}
}
@book{Strang2019,
	author = {Gilbert Strang},
	isbn = {9780692196380},
	pages = {432},
	publisher = {Wellesley-Cambridge Press},
	title = {Linear algebra and learning from data},
	year = {2019}
}
@book{Hefferson2020,
	author = {Jim Hefferon},
	edition = {4},
	publisher = {Orthogonal Publishing L3C},
	title = {Linear Algebra},
	year = {2020}
	url = {http://joshua.smcvt.edu/linearalgebra}
}
@article{Engler1997,
	abstract = {A bound on the performance of QR-factorization with column pivoting is derived and two classes of matrices are constructed for which the bound is sharp or asymptotically sharp.},
	author = {Hans Engler},
	issue = {6},
	journal = {Applied Mathematics Letters},
	keywords = {Pivoting strategy,QR-factorization,Subset selection},
	month = {7},
	title = {The Behavior of the QR-Factorization Algorithm with Column Pivoting},
	volume = {10},
	year = {1997}
}
@article{CHAN198767,
	title = {Rank revealing QR factorizations},
	journal = {Linear Algebra and its Applications},
	volume = {88-89},
	pages = {67-82},
	year = {1987},
	issn = {0024-3795},
	doi = {https://doi.org/10.1016/0024-3795(87)90103-0},
	url = {https://www.sciencedirect.com/science/article/pii/0024379587901030},
	author = {Tony F. Chan},
	abstract = {An algorithm is presented for computing a column permutation Π and a O̧R factorization AΠ = QR of an m by n (m⩾n) matrix A such that a possible rank deficiency of A will be revealed in the triangular factor R having a small lower right block. For matrices of low rank deficiency, the algorithm is guaranteed to reveal the rank of A, and the cost is only slightly more than the cost of one regular O̧R factorization. A posteriori upper and lower bounds on the singular values of A are derived and can be used to infer the numerical rank of A.}
}

@book{Kutner2005,
	abstract = {Fifth edition. CD-ROM Title: Student CD-ROM to accompany applied linear statistical models, 5th ed. Simple Linear Regression -- Multiple Linear Regression -- NonLinear Regression -- Design and Analysis of Single-Factor Studies -- Multi-Factor Studies -- Specialized Study Designs.},
	author = {Michael H. Kutner and Chris Nachtsheim and John Neter and William Li},
	isbn = {0072386886},
	pages = {1396},
	publisher = {McGraw-Hill/Irwin},
	title = {Applied linear statistical models},
	year = {2005}
}
@book{Belsley2004,
	author = {David Belsley and Edwin Kuh and Roy Welsch},
	isbn = {0-471-69117-8},
	publisher = {John Wiley \& Sons, Inc.},
	title = {Regression Diagnostics},
	year = {2004}
}
