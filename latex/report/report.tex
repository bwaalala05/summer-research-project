\documentclass[12pt]{article}
\usepackage{fontspec}
\setmainfont{Times New Roman} 


% PREAMBLE
\usepackage{amsthm}
\newtheorem{prop}{Proposition}[section]
\newtheorem{dfn}{Definition}[section]
\renewcommand{\refname}{References}
\input{preamble2.tex}
\input{macros.tex}
\input{letterfonts.tex}
\usepackage{graphicx}
\usepackage{setspace}
\setlength{\parindent}{0pt}
\DeclareMathOperator{\Cn}{\CC^n}
\DeclareMathOperator{\Th}{^{th}}
\DeclareMathOperator{\Fn}{\FF^n}
\DeclareMathOperator{\ima}{Im}
\DeclareMathOperator{\col}{col}
\DeclareMathOperator{\Col}{Col}
\DeclareMathOperator{\proj}{proj}
\DeclareMathOperator{\colsp}{Colspan}
\DeclareMathOperator{\Colspan}{Colspan}
\DeclareMathOperator{\Rowspan}{Rowspan}
\DeclareMathOperator{\rank}{rank}
\DeclareMathOperator{\nul}{nullity}
\DeclareMathOperator{\Hom}{Hom}
\DeclareMathOperator{\Nul}{Nul}
\DeclareMathOperator{\diag}{diag}
\DeclareMathOperator{\re}{Re}
\DeclareMathOperator{\Span}{span}
\DeclareMathOperator{\SSE}{SSE}
\DeclareMathOperator{\mySS}{SS}
\DeclareMathOperator{\MSE}{MSE}
\DeclareMathOperator{\RSS}{RSS}
\DeclareMathOperator{\Var}{Var}
\DeclareMathOperator{\var}{\sigma^2}
\DeclareMathOperator{\vif}{VIF}
\newcommand{\cL}{\mathcal{L}}
\newcommand{\cM}{\mathcal{M}}
\newcommand{\cP}{\mathcal{P}}
\newcommand{\Sg}{\Sigma}
\newcommand{\gbh}{\hat{\gb}}
\newcommand{\adjR}{R_a^2}
\newcommand{\res}{\hat{\epsilon}}
\newcommand{\bmgb}{\boldsymbol{\gb}}
\setstretch{1.15}

\title{Exploring the least squares solver of R and Python}
\author{Lim Zi Xiang}
\date{}

\begin{document}
	%\maketitle
	\pdfbookmark[section]{\contentsname}{toc}
	\tableofcontents
	\pagebreak
	
	\section{Introduction}
	
	Multiple linear regression is widely used in many applications to model linear relationships between a set of predictors and a response. One of the more common and simpler methods of fitting such a model is known as the ordinary least squares method (OLS), which is found to be a Best Linear Unbiased Estimator (BLUE) by the Gauss-Markov theorem, that is, it gives the unbiased estimates of each regression coefficient with the lowest possible variance, provided that assumptions are satisfied. Furthermore, it has a main advantage of it being easily interpretable as a parametric model as opposed to non-parametric models.\\
	
	\setlength\parindent{24pt} The motivation for this project arises from one of these assumptions, that is, the data matrix, $\bmX$, is full rank, i.e. none of the predictors are (nearly) perfectly correlated with each other. In cases where this is not true, OLS is unable to estimate regression parameters due to singularity. In such cases, there are several methods we can employ to address this issue, whether to drop one of the correlated variables, or to combine them, etc., based on the discretion of the data analyst. In this project, we investigate how two of the commonly used software for regression analysis, Python (\texttt{scikit\_learn} and \texttt{statsmodel}) and R each handles perfect collinearity when fitting the model.\\
	
	Furthermore, we embrace this opportunity to explore further on the topic of multicollinearity, such as its impact on model prediction and interpretation. We also apply several known methods of detecting collinearity in the data, whether through hypothesis testing, or by using measures such as the variance inflation factor (VIF) and condition numbers/indices. In addition to aforementioned methods of addressing collinearity, we consider shrinkage methods such as ridge regression and lasso to alleviate the impact of overfitting (due to collinearity) on variance at the cost of negligible bias. We also briefly explore another potential pitfall in regression analysis, that is confounding variables, by examining examples of Simpson's paradox.\\
	
	Here, we lay down the general setup for fitting a linear regression model. Given an $n\times p$ data matrix $\bmX$ ($n$ samples and $p$ predictors), corresponding to an $n\times 1$ column matrix $\bmy$ containing the observed response, we model the relationship between $p$ predictors, $X_i$  and the response variable, $Y$ as
	$$Y=\beta_0+\sum_{i=1}^{p}\beta_i X_i+\varepsilon,$$
	where $\varepsilon$ represents the random error. We fit the model to the data collected for the expected response $E[Y]$ by estimating each $\beta_i$ using the least squares method to obtain $\gbh_i$ so that we have
	$$E[Y]=\gbh_0+\sum_{i=1}^{p}\gbh_iX_i.$$
	
	Finally, it should be noted that there are different conventions to what collinearity and multicollinearity mean according to varying sources. In this report, we will use these two terms interchangably to refer to the correlation between two or more predictors in a regression model.
	
	\section{Objectives}
	
	There are three main objectives for this project:
	
	\begin{enumerate}
		\item Explore issues that arise from singularity.
		\item Understand singular value decomposition (SVD).
		\item Investigate how software deals with singularity when building linear models.
	\end{enumerate}
	
	In practical settings, it is unlikely to obtain a set of perfectly correlated data due to inevitable noise. However, even near perfect collinearity leads to singularity when attempting to fit the regression model using software. We hence aim to explore how a rank-deficient data matrix can lead to singularity and the issues that arise from it. We also explore pitfalls caused by moderate to high multicollinearity in terms of interpretation and prediction. Subsequently, we look into commonly used methods to handle situations where regressors are highly correlated depending on purpose of analysis.\\
	
	The next objective is to understand how to apply the singular value decomposition (SVD) in the computations of the least square estimates. The least squares method estimates the regression coefficients of a model with $p$ predictors by fitting it to a dataset with $n$ observations using the following equation
	\begin{equation} \label{eq1}
		\boldsymbol{\gbh}=(\bmX^T\bmX)^{-1}\bmX^T\bmy,
	\end{equation}
	where $\boldsymbol{\gbh}$ is a $p\times 1$ column matrix containing the least square estimates, $\bmX$ is a $n\times p$ data matrix, and $\bmy$ is a $n\times 1$ column matrix containing the observed response values. The derivation of this formula is included in the appendix (\hyperref[sec:append1]{Section B.1}). With the presence of perfect collinearity, the matrix $\bmX^T\bmX$ is singular and hence $(\bmX^T\bmX)^{-1}$ does not exist. However, the least squares solver of Python uses the Moore-Penrose inverse (or pseudoinverse) of $\bmX$, $\bmX^+$, which can be used to calculate the minimum norm approximation of $(\bmX^T\bmX)^{-1}$, and can be computed using the SVD of $\bmX^T\bmX$ (see appendix \hyperref[sec:append2]{Section B.2}). A brief section in the appendix (\hyperref[sec:svd]{Section A}) is dedicated to the formulation and computation of the SVD.\\
	
	Lastly, it is observed that there exists discrepancy between how different software handles highly correlated predictors when fitting a multiple linear regression model via OLS. In this project, we aim to understand the reason behind this difference in behaviour by referring to standard documentation and examining program output in Python and R. As an extension, methods for detecting and addressing multicollinearity using software are also explored.
	
	\section{Methodology}
	
	We narrowed our focus to three least squares solvers of interests: Python's \texttt{scikit-learn} library, Python's \texttt{statsmodel} library, and R's \texttt{lm} function. In each case, we randomly generated several sets of artificial data using three regressors where only one pair is correlated, each with 100 samples. The use of artificial data has the advantage where the true relationship between the response and predictors is known and self-determined, and that the degree of correlation and noise in the data can be manually specified. Note that the distribution used in data generation is irrelevant for our topic of study.\\
	
	The data ranges from 0 to 100 and is uniformly distributed. The degree of collinearity was then varied between perfect and moderate to observe differences in output. The pseudoinverse of the data matrix was also computed and multiplied with the generated response to give the theoretical minimum norm solution for the coefficient estimates. Then, these results were compared with those given by the least squares solver function to confirm whether these functions use the pseudoinverse of the data matrix to estimate the least squares coefficients as suggested by documentation.\\
	
	The true underlying relationship to which the artificial data was generated was set to be 
	$$Y=X_1+2X_2+3X_3,$$
	where $X_1$ and $X_2$ are correlated with $X_2=2X_1$. We then vary the amount of uniformly distributed noise added to different generated sets: from no noise (perfect collinearity) to some number sampled from intervals [-25,25], [-50,50], and [-100,100]. The train-test ratio chosen for splitting the dataset into training and testing sets is fixed at 80-20. Lastly, to evaluate prediction performance, metrics such as the (adjusted) multiple coefficient of determination $R^2$ and mean squared error were used.\\
	
	The scaling effect of \texttt{scikit\_learn}'s \texttt{StandardScaler} on the ridge regression and LASSO models in the presence of collinearity was also given attention. This was done by referring to standard documentation as well as by testing it using artificially generated data. For data with and without high multicollinearity present respectively, two models were each fitted: one with predictors scaled by \texttt{StandardScaler} and one without. The output was then compared. \\
	
	We have also performed regression analysis on California housing and applied methods to address collinearity using \texttt{statsmodel}. Subsequently, ridge regression models and lasso models were also used to examine its effectiveness in decreasing the standard errors of these models.
	
	\subsection{Methods to detect collinearity}
	
	One simple way to detect collinearity is through a test for correlation, where we test the hypothesis
	$$H_0: \rho=0 \quad\quad\text{ vs. }\quad\quad H_1: \rho\neq 0,$$
	where $\rho$ is the population correlation coefficient between a chosen pair of predictors. However, there are two main reasons to opt for an alternative method.
	
	Firstly, as the number of predictors included in the model increases, the number of tests required to cover every combination of pairs increases. With large number of tests, the probability that we commit at least one error is high and hence we risk incorrectly identifying correlated pairs. More importantly, even if we find that every pair of predictors has no significant correlation, we cannot make the same conclusion about predictors amongst groups of other numbers. 
	
	\subsubsection{Variance inflation factor (VIF)}
	A commonly used method for identifying highly correlated predictors is by using the variance inflation factor (VIF), given by
	$$\vif_k=\frac{1}{1-R_{k}^2},$$
	where $R^2_k$ is the multiple coefficient of determination of the $k^{\text{th}}$ predictor regressed against the other predictors, assuming that it has zero mean. Intuitively, a higher value of $R^2_k$ indicates a good fit, hence a strong correlation between the $k^{\text{th}}$ regressor and some of the other regressors. Note that uncorrelated regressors would simply have a coefficient close to zero. \\
	
	The motivation behind the formulation of the VIF is to calculate the ratio between the variance of a coefficient estimate if it was uncorrelated with the other regressors (baseline case), and the actual variance. For example, a coefficient estimate having a VIF of 4 corresponds to its variance being 4 times of what it would have been if it was uncorrelated with every other predictor. In other words, VIF measures the factor by which the variance of the regression coefficients is inflated above what it would have been if $R_k^2=0$, hence its name. \\
	
	There are different rule of thumbs for the thresholds beyond which one may consider the VIF value of a particular predictor to be high, ranging roughly from 4 to 10. There is no definitively correct answer, and we have no choice but to make decisions based on our interpretation of the VIF value. Once predictors with high VIF's are identified, we can then examine the correlation matrix to identify which the predictors which each high VIF predictor is correlated to.
	
	\subsubsection{Condition numbers}
	Another metric for measuring the degreee of collinearity is the condition number (condition indices) of the data matrix $\bmX$, which indicates how sensitive the response is to changes in $\bmX$. Aside from using it to determine whether a model is highly collinear, the condition number of $\bmX^T\bmX$ is used to check for a potential source of large numerical error due to collinearity when computing $(\bmX^T\bmX)^{-1}$ using software. In this case, it indicates the multiplicative factor to which round-off errors are amplified due to computer arithmetic being performed to finite precision.\\
	
	The formula for the condition number of a $m\times n$ matrix $\bmA$ with rank $r\leq n$, denoted $\kappa(\bmA)$, is given by
	$$\kappa(\bmA)=\frac{\sigma_1}{\sigma_r},$$
	where $\sigma_i$ are the singular values of $\bmA$ arranged in descending order. Note that definitions will differ for varying contexts, giving rise to different condition numbers. The point of discussion here is the condition number for inversion.\\
	
	Similar to the case of VIF, there is no exact threshold for a condition number to be considered high, but a rule of thumb is any condition numbers exceeding 20 indicates the matrix to be ill-conditioned. 
	
	\subsection{Methods to address collinearity}
	
	We present a few commonly-used methods to address moderate to perfect collinearity. \\
	
	In the case of perfect collinearity, one of the correlated predictors is completely described by the other, hence our only choice is to remove one of them. This is however not recommended for non-perfect collinearity as we are at risk of committing omitted variable bias. In such cases, we may want to either combine some of the variables or use shrinkage methods such as ridge regression or LASSO.\\
	
	There are many possible ways of combining correlated variables, but in most cases the simple approach of adding the columns up (such as what R does) gives sufficiently improved results. Note that it is important that we consider the context when doing this, that is, whether combining the two variables make sense in context. An example is a model predicting the salary of employees based on predictors such as years of experience, gender, test scores for course A, and test scores for course B. Obviously, any employee who does well on one test is expected to do well in the other, and so we may consider adding the two test scores together to form a single column "total test scores for course A and B", then the results obtained may be interpreted accordingly.\\
	
	%When analysing the California housing dataset, we identified two pairs of correlated predictors: \texttt{AveBedrms} with \texttt{AveRooms} (), and \texttt{Longitude} with \texttt{Latitude}. \\
	
	For ridge regression and LASSO, otherwise known as $L_2$ and $L_1$ regularisation respectively, we find that it successfully decreases coefficient standard errors at the cost of increasing the bias (and hence decreasing $R^2$ score compared to the OLS model) by a neglibible amount. However, in the face of perfect collinearity, ridge and lasso does not yield interpretable coefficients, where the correlated predictors have equally distributed coefficients due to the effect of standardising the predictors beforehand (see Appendix \hyperref[sec:append2]{Section B.2}), as the ridge and LASSO coefficient estimates are not scale equivariant. 
	
	
	\section{Results and discussion}
	\subsection{Python's \texttt{scikit-learn} library}
	
	After referring to documentation, we find that the least squares solver, \texttt{LinearRigression().fit()} computes the coefficient estimates using the pseudoinverse of the data matrix, which we have verified. Under any degree of collinearity, we observed that model prediction is affected minimally, attributed by $R^2$ scores close to 1. However, when predictors are scaled, the ridge regression or LASSO model fitted to data with perfect collinearity results in equal distributed coefficients between correlated predictors. After further investigation, we discovered that this was caused by the behaviour of \texttt{StandardScaler} from the same library which was used to scale predictors prior to fitting the model, which we show in \hyperref[sec:append4]{Section B.4}.\\
	
	We also note that the solver does not give any warnings of high collinearity when displaying results and does not apply any methods to handle highly collinear data.
	
	\subsection{Python's \texttt{statsmodel} library}
	(include screenshot of results?)
	\begin{itemize}
		\item Looks like it evenly distributes weights throughout the correlated predictors.
		\item Uses pseudoinverse
		\item Gives warning upon detect of high to perfect collinearity.
	\end{itemize}
	
	\subsection{R's \texttt{lm} function}
	(include screenshot of results?)
	
	\begin{itemize}
		\item Looks like it dropped one collinear column and add the two coefficients together.
		\item Seems to combine collinear columns.
		\item Gives a warning when highly correlated variables are detected
		\item Option available to abort process upon detection of high mutlicollinearity.
	\end{itemize}
	
	\subsection{Impact of multicollinearity}
	
	High multicollinearity mainly poses a major problem to model interpretation. Notably, it inflates the variance and covariances of least squares estimates and causes estimates and their variance to become sensitive to minor changes in data. As a result, we may obtain different results when performing variable selection after removing or adding samples from the dataset, or obtain incorrect signs of cofficients estimates. Another curious effect is that the $t$-values of correlated predictors may be insignificant yet removing these predictors from the model results in a lower $R^2$ value. Although this seems contradictory by intuition, it is actually reasonable behaviour, as the prediction is contributed by both variables while the $t$-values indicate the contribution of one variable after accounting for the others.\\
	
	When two predictors have perfect correlation, we have what is known as perfect collinearity. In such a case, at least one of the columns of the data matrix $\bmX$ is a linear combination of the other columns. This causes the matrix $\bmX^T\bmX$ to be singular (see appendix \hyperref[sec:append1]{Section B.3}), consequently causing $\boldsymbol{\gbh}$ unable to be computed (see \hyperref[eq1]{Equation 1}). However, \texttt{statsmodel}'s and \texttt{scikit\_learn}'s least square solvers compute the pseudoinverse of $\bmX^T\bmX$ when estimating the regression coefficients, giving the least norm approximated solution (which is interpretable) when $\bmX$ is rank-deficient. However, the aforementioned impacts of high collinearity still apply hence it should be addressed.\\
	
	It is important to note that collinearity does not, in any way, impact predictions or its precision. Hence, if the model is not expected to be interpreted, multicollinearity is no issue of concern and may be ignored. Moreover, even for more statistically-oriented analysts, there may also not be a need to address high collinearity provided that it does not inflate standard errors to a problematic extent (see "A caution regarding rules of thumb for variance inflation factors" by O'Brien), as there are many other factors which can suppress the inflation effect of standard errors, one of which is the sample size. Quoting the words of O'Brien, "collinearity does not hurt as long as it does not bite".
	
	
	
	
	\subsection{Confounding variables}
	
	In addition to the pitfall of collinearity in performing regression analysis, confounding variables, or confounders, can also severely distort our results if not given attention. The concept of confounders is illustrated using Simpson's paradox, where the relationship between individual predictors and the response disappears or reverses when all predictors are included in a single model. The variables which cause such occurences are referred to as confounders.\\
	
	We give the classic textbook example of the relationship between ice cream sales and shark attacks. We initially observe a positive correlation between these two variables, yet upon the inclusion of the confounder, temperature, we find that this correlation vanishes. One should avoid the confusion between confounders and correlated variables by noting that in addition to being correlated, confounders should also have a causal relationship to the variables involved in the confounding effect.\\
	
	Although including confounders inherently introduces collinearity into the model, failing to account for them causes us to commit omitted variable bias in addition to causing the aforementioned confounding effects according to Simpson's paradox. (We note the difference between these confounding effects and the effect of collinearity on the coefficient signs: the latter may still occur even after including confounders in the model due to high collinearity causing coefficient estimates to become sensitive to changes in observed data). Hence, by weighing each consequence, we generally always include confounders as we have well-established methods for effectively addressing collinearity. Unfortunately, it is more often the case that confounders are excluded unintentionally, as there is no standard method of identifying them. Furthermore, this is not a problem exclusive to regression modelling, but observational studies in general. Hence, we highlight this further in \hyperref[sec:future-rec]{Section 6}.
	
	\section{Conclusion}
	
	After examining Python's and R's program output, we have observed differences in how they each handle the issue of multicollinearity. In particular, R's behaviour of combining correlated predictors while giving a warning upon detection of high multicollinearity aligns best with standard practice. It is also found that in the process of estimating the regression coefficients, R's \texttt{lm}, Python's \texttt{statsmodel}, and \texttt{scikit\_learn} calculates the pseudoinverse of $\bmX^T\bmX$ rather than $(\bmX^T\bmX)^{-1}$ so that a result may be obtained even in the presence of singularity.\\
	
	Additionally, we have observed a difference between the three's behaviour of flagging problematic multicollinearity, which indicates the general attitude of different fields that uses regression modelling. For example, \texttt{scikit\_learn} aligns with machine learning engineers who are generally more concerned about making accurate predictions and hence would not be as concerned about the impacts of collinearity on model interpretability. This is in stark contrast to statisticians who bear greater interest in investigating the relationships between the regressors and the response. Hence, as opposed to \texttt{scikit\_learn}, they would be more likely to use \texttt{statsmodel} or R, which in addition to flagging high multicollinearity, also affords a more extensive function library for the purpose of performing analysis on the model.\\
	
	Taking these distinctions into account, it depends ultimately on the data analyst to make the final decision on which approach is best to handle anomalies in the model after performing analysis according to context and domain knowledge. Nevertheless, it is still crucial that one is made aware of these differences so as to select the appropriate tools depending on the individual's purpose and objective of regression modelling. 
	
	\section{Future recommendations\label{sec:future-rec}}
	\begin{enumerate}
		\item Explore the same issue in other software commonly used in the industry such as (?)
		\item Confounding and causality.
		\item Partial effect, total effect, direct effect
	\end{enumerate}
	
	\pagebreak
	
	%%------------------------------------------------
	% if you need an appendix
	\appendix %enables appendix numbering
	\section{\label{sec:svd}Singular value decomposition}
	The SVD is a decomposition of any matrix into a sum of rank-1 matrices. Before we give its formulation, we first state several definitions as well as results without proof.
	\subsection{Prerequisites}
	\begin{dfn}[Symmetric matrices]
		An $n\times n$ square matrix $\bmX$ is symmetric if $\bmA=\bmA^T$.
	\end{dfn}
	\begin{dfn}[Orthogonal matrices]
		An $n\times n$ square matrix $\bmX=\begin{pmatrix}
			\bmx_1 & \ldots & \bmx_n
		\end{pmatrix}$ is orthogonal if its columns $\bmx_i$ are orthonormal, that is, they are orthogonal to each other and each have a norm of 1, i.e. $\bmx_i\bmx_j^T=0$ for $i\neq j$ and $\bmx_i\bmx_j^T=1$ for $i=j$, where $1\leq i,j\leq n$.
	\end{dfn}
	\begin{prop}[Spectral/eigen-decomposition]
		Any $n\times n$ symmetric matrix $\bmA$ can be written as 
		$$\bmA=\bmQ\boldsymbol{\Lambda}\bmQ^T=\sum_{i=1}^{n}\lm_i\bmq_i\bmq_i^T,$$
		where $\boldsymbol{\Lambda}=\diag\{\lm_1,\ldots,\lm_n\}$ is an $n\times n$ diagonal matrix with diagonals equal to the eigenvalues of $\bmA$ and $\bmQ$ is an orthogonal matrix whose columns are unit eigenvectors $\bmq_1,\ldots,\bmq_n$ of $\bmA$.
	\end{prop}
	\begin{dfn}[Singular vectors and singular values]
		Let $\bmA$ be a $n\times p$ matrix, then $\sigma$ is a singular value with corresponding left and right singular vectors $\bmu$ and $\bmv$ respectively if
		$$\bmA\bmv=\sigma\bmu \quad \text{ and }\quad \bmA^T\bmu=\sigma\bmv.$$
	\end{dfn}
	
	\subsection{Singular value decomposition}
	\begin{prop}[Singular value decomposition]
		Let $\bmA$ be an $n\times p$ matrix with rank $r$, where $1\leq r\leq \min(n,p)$. Then there exists an $n\times r$ matrix $\bmU=\{\bmu_1,\ldots,\bmu_r\}$, an $p\times r$ matrix $\bmV=\{\bmv_1,\ldots,\bmv_r\}$, and an $r\times r$ matrix $\boldsymbol{\Sg}=\diag\{\sigma_1,\ldots,\sigma_r\}$, such that
		$$\bmA=\bmU\boldsymbol{\Sg}\bmV^T=\sum_{i=1}^{r}\sigma_i\bmu_i\bmv_i^T,$$
		where $\bmU\bmU^T=\bmI_r=\bmV\bmV^T$, and $\sigma_1\geq\ldots\geq\sigma_r>0$.
	\end{prop}
	
	The form given above is called the compact SVD. The non-compact form is given by
	$$\bmA=\bmU\boldsymbol{\Sg}\bmV^T,$$
	where $\bmU$ is an $n\times n$ orthogonal matrix, $\bmV$ is a $p\times p$ orthogonal matrix and $\Sg$ is a $n\times p$ diagonal matrix, leaving the remaining diagonals after the $r^{\text{th}}$ entry zero.

	\begin{prop}
		Let $\bmA$ be an $n\times p$ matrix with rank $r$, then 
		$$\rank(\bmA^T\bmA)=\rank(\bmA)=r.$$
	\end{prop}
	
	\begin{proof}
		To prove this, we show that $\bmA$ and $\bmA^T\bmA$ have the same null space and hence the same nullity. We do this by proving that $\nul(\bmA)\subseteq\nul(\bmA^T\bmA)$ and $\nul(\bmA^T\bmA)\subseteq\nul(\bmA)$ separately.\\
		
		For the first case, let $\bmx\in\RR^p$ such that $\bmA\bmx=\bzero_n$. Then, $\bmA^T\bmA\bmx=\bmA^T(\bzero_n)=\bzero_r$. So $\nul(\bmA)\subseteq\nul(\bmA^T\bmA)$. For the second case, $\bmx\in\RR^p$ such that $\bmA^T\bmA\bmx=\bzero$. Left multiplying both sides by $\bmx^T$, we have
		$$\bmx^T\bmA^T\bmA\bmx=(\bmA\bmx)^T(\bmA\bmx)=\|\bmA\bmx\|^2=\bzero,$$
		which implies that $\bmA\bmx=\bzero$, so $\nul(\bmA^T\bmA)\subseteq\nul(\bmA)$.\\

		Then since the dimension of the domain of $\bmA^T\bmA$ and $\bmA$ are both $p$, by the rank-nullity formula, they have the same rank.
	\end{proof}
	
	\begin{prop}\label{prop1}
		Let $\bmA$ be an $n\times p$ matrix of rank $r$. Then the non-zero eigenvalues of both $\bmA\bmA^T$ and $\bmA^T\bmA$ are $\sigma_1^2,\ldots,\sigma_r^2$.
		Furthermore, the corresponding unit eigenvectors of $\bmA\bmA^T$ and $\bmA^T\bmA$ are given by the columns of $\bmU$ and $\bmV$ respectively.
	\end{prop}
	
	\begin{proof}
		We prove this result by first noting that $\bmA^T\bmA$ is a $p\times p$ symmetric matrix, so we can write its spectral decomposition as
		$$\bmA^T\bmA=\bmV\boldsymbol{\Lambda}\bmV^T$$
		where $\bmV$ is a $p\times r$ {semi-orthogonal} matrix containing the orthonormal eigenvectors of $\bmA^T\bmA$ (and hence its columns $\bmv_i$ are orthonormal) and $\boldsymbol{\Lambda}=\diag(\lm_1,\ldots,\lm_r)$ is a diagonal matrix with its diagonals containing eigenvalues $\lm_1\geq\ldots\geq\lm_r>0$. Note that we know that there are $r$ eigenvalues by \hyperref[prop1]{Proposition A.3}. \\
		
		Now, we let each $\sigma_i=\sqrt{\lm_i}$ and $\bmu_i=\frac{1}{\sigma_i}\bmA\bmv_i$ for $i=1,\ldots,r$. We then show that the vectors $\bmu_i$ are orthonormal and subsequently, $\bmu_i$ and $\bmv_i$ are left and right singular vectors corresponding to singular values $\sigma_i$ as follows:
		\begin{align*}
			\bmu_i^T\bmu_j
			=\frac{1}{\sigma_i\sigma_j}\bmv_i^T{\bmA^T\bmA\bmv_j}
			=\frac{1}{\sigma_i\sigma_j}\bmv_i^T({\lm_j\bmv_j})
			=\frac{\sigma_j^2}{\sigma_i\sigma_j}\bmv_i^T\bmv_j
			=\frac{\sigma_j^2}{\sigma_i\sigma_j}\bmv_i\cdot\bmv_j.
		\end{align*}
		Notice that if $i=j$, then the expression simplifies to just $\|\bmv_i\|^2=1$ since the vectors $\bmv_i$ are orthonormal. If $i\neq j$, then the dot product equals to zero, implying that the vectors $\bmu_i$ are orthonormal. Now, following the definition of singular vectors, we consider
		$$\bmA^T{\bmu_i}={\frac{1}{\sigma_i}}(\bmA^T{\bmA\bmv_i})=\frac{{\sigma_i^2}}{\sigma_i}{\bmv_i}=\sigma_i\bmv_i,$$
		so $\bmv_i$ are right singular vectors corresponding to singular values $\sigma_i$ while $\bmu_i$ by our formulation are left singular vectors. Now all that is left is to construct $\bmU=\begin{pmatrix}
			\bmu_1 & \ldots & \bmu_r & \ldots & \bmu_n
		\end{pmatrix}$ and $\boldsymbol{\Sg}=\diag\{\sigma_1,\ldots,\sigma_r,0,\ldots,0\}$. 
	\end{proof}
	
	\subsection{Computation of the SVD}
	Let $\bmA$ be any $m\times n$ matrix. To compute the SVD of $\bmA$, we first solve for eigenvalues $\lm$ of $\bmA\bmA^T$ or $\bmA^T\bmA$ depending on which results in a matrix of smaller size. Then we take the singular values $\sigma=\sqrt{\lm}$ and construct $\boldsymbol{\Sg}$. Since in regression modelling, we often have more samples than predictors (which translates to more rows than columns), we choose $\bmA^T\bmA$. Now, we solve for right singular vectors by using its definition:
	$$(\bmA^T\bmA-\lm I)\bmv=\bzero.$$
	Then, we convert the right singular vectors to unit vectors (so that $\bmU$ is orthogonal) and construct matrix $\bmU$. Compute $\bmV$ by using the definition of right singular vectors:
	$$\sigma_i\bmv_i=\bmA^T\bmu_i.$$
	
	\section{Referenced results}
	\subsection{Fitting the OLS model\label{sec:append1}}
	(explain using the concept of orthogonal projection)
	
	\subsection{Minimum norm approximation using pseudoinverse\label{sec:append2}}
	We can compute the pseudoinverse of $\bmX^T\bmX$ using its SVD as follows.\\
	
	Let $\bmA=\bmX^T\bmX$. If the SVD of $\bmA=\bmU\Sg\bmV^T$, then the pseudoinverse of $\bmA$, $\bmA^+$, is given by
	\begin{align*}
		\bmA^+&=((\bmU\Sg\bmV^T)^T()\bmU\Sg\bmV^T)^{-1}(\bmU\Sg\bmV^T)^T\\
		&=(\bmV\Sg\bmU^T\bmU\Sg\bmV^T)^{-1}(\bmU\Sg\bmV^T)^T\\
		&=(\bmV\Sg^2\bmV^T)^{-1}(\bmV\Sg\bmU^T)\\
		&=(\bmV^T)^{-1}(\Sg^{-1})^2(\bmV)^{-1}\bmV\Sg\bmU^T\\
		&=\bmV\Sg^{-1}\bmU^T
	\end{align*}
	
	Now, for $\bmA\in\RR^{m\times n}$ and $\bmb\in\RR^m$, if the linear system $\bmA\bmx=\bmb$ has solutions, then $\bmx^*=\bmA^+\bmb$ is an exact solution and has the smallest possible norm, i.e. $\|\bmx^*\|\leq\|\bmx\|\, $for all $\bmx$.\\
	
	The proof is as follows: Since $\bmA^+$ is a generalised inverse, $\bmA^+\bmb$ must be a solution to $\bmA\bmx=\bmb$. Now, for any solution $\bmx\in\Rn$, consider its orthogonal decomposition via $\bmA^+\bmA\in\RR^{n\times n}$:
	$$\bmx=(\bmA^+\bmA)\bmx+(\bmI-\bmA^+\bmA)\bmx=\bmA^+\bmb+(\bmI-\bmA^+\bmA)\bmx.$$
	Then by the Pythagorean theorem, we have
	$$\|\bmx\|^2=\|\bmA^+\bmb\|^2+\|(\bmI-\bmA^+\bmA)\bmx\|^2\geq \|\bmA^+\bmb\|^2.$$
	Hence $\|\bmx\|\geq\|\bmA^+\bmb\|$.
	
	\subsection{Singularity due to perfect collinearity \label{sec:append3}}
	Let $\bmX=\begin{pmatrix}
		\bmu & k\bmu & \bmv
	\end{pmatrix}$ where $\bmu,\bmv\in\Rn$ are arbitrarily linearly independent column vectors containing $n$ observations of a particular feature with $\bmu^T=\begin{pmatrix}
		u_1 & \ldots & u_n
	\end{pmatrix}$ and $\bmv^T=\begin{pmatrix}
		v_1 & \ldots & v_n
	\end{pmatrix}$, with $k\in\RR$ such that the column $k\bmu$ is a scalar multiple of the first column. Consider
	\begin{align*}
		\bmX^T\bmX&=\begin{pmatrix}
			\bmu^T \\ k\bmu^T \\ \bmv^T
		\end{pmatrix}\begin{pmatrix}
			\bmu & k\bmu & \bmv
		\end{pmatrix}\\
		&=\begin{pmatrix}
			u_1 & \ldots & u_n \\ ku_1 & \ldots & ku_n \\ v_1 & \ldots & v_n
		\end{pmatrix}\begin{pmatrix}
			u_1 & ku_1 & v_1 \\ \vdots & \vdots & \vdots \\ u_n & ku_n & v_n
		\end{pmatrix}\\
		&=\sum_{i=1}^{n}\begin{pmatrix}
			u_i \\ ku_i \\ v_i
		\end{pmatrix}\begin{pmatrix}
			u_i & ku_i & v_i
		\end{pmatrix}\\
		&=\sum_{i=1}^{n}\begin{pmatrix}
			u_i^2 & ku_i^2 & u_iv_i \\ 
			ku_i^2 & k^2u_i^2 & ku_iv_i \\
			u_iv_i & ku_iv_i & v_i^2
		\end{pmatrix}.
	\end{align*}
	The second row of $\bmX^T\bmX$ is $k$ times the first row for every $i$. After summing up the rank-1 matrices, the second row of $\bmX^T\bmX$ will also be $k$ times the first. So, $\bmX^T\bmX$ is not full rank, and hence is singular. \\
	
	By writing matrix-matrix products in this form, each term in the sum is a rank-1 matrix. However, under no collinearity, the rows in every term will not necessarily be dependent on some other rows \textbf{by the same factor}. In the formulation above, the third row differs from the first row by a multiplicative factor of $v_i/u_i$ (this is consistent with the case of collinear columns since $ku_i/u_i=k$), which is a constant under no collinearity. This fraction will differ for varying $i$, thus after adding each term up, there is no common factor between the two rows.\\
	
	Alternatively for a simpler and less analytic approach, we look at each entry in the product matrix as multiplying rows with columns:
	
	$$\bmX^T\bmX=\begin{pmatrix}
		\bmu\cdot\bmu & \bmu\cdot(k\bmu) & \bmu\cdot\bmv\\
		(k\bmu)\cdot\bmu & (k\bmu)\cdot(k\bmu) & (k\bmu)\cdot\bmv \\
		\bmv\cdot\bmu & \bmv\cdot(k\bmu) & \bmv\cdot\bmv
	\end{pmatrix}=\begin{pmatrix}
		\bmu\cdot\bmu & k\bmu\cdot\bmu & \bmu\cdot\bmv\\
		k(\bmu\cdot\bmu) & k(k\bmu\cdot\bmu) & k(\bmu\cdot\bmv) \\
		\bmv\cdot\bmu & k(\bmv\cdot\bmu) & \bmv\cdot\bmv
	\end{pmatrix}.$$
	
	We observe that the second row is $k$ times the first row as expected. The third row again is not collinear with the first two. Suppose they are, then we solve for some $\lm\in\RR$ such that
	$$\bmv\cdot\bmu=\lm(\bmu\cdot\bmu) \quad \text{and}\quad \bmv\cdot\bmv=\lm(\bmu\cdot\bmv).$$
	It then follows that
	\begin{align*}
		\|\bmv\|^2&=\bmv\cdot\bmv\\
		&=\lm(\bmv\cdot\bmu)\\
		&=\lm^2(\bmu\cdot\bmu)\\
		&=\lm^2\|\bmu\|^2\\ 
		\|\bmv\|&=\lm\|\bmu\|\\
		&=\dfrac{\bmv\cdot\bmu}{\|\bmu\|^2}\|\bmu\|\\
		\bmv&=\dfrac{\bmv\cdot\bmu}{\|\bmu\|^2}\bmu
	\end{align*}
	$$$$
	i.e. $\bmv$ is its own orthogonal projection on vector $\bmu$, implying that they are collinear, which is a contradiction. Hence the third row is linearly independent of the first two rows. \\
	
	Since the least squares coefficient estimates are given by
	$$\boldsymbol{\gbh}=(\bmX^T\bmX)^{-1}\bmX^T\bmy$$
	the singularity of $\bmX^T\bmX$ causes least squares to fail to obtain estimates for .
	
	\subsection{Effect of \texttt{StandardScaler} on collinear columns in data matrix \label{sec:append4}}
	From the documentation, \texttt{scikit\_learn}'s \texttt{StandardScaler} calculates the $Z$-score of each entry by subtracting each entry by the mean of its column and dividing by the standard deviation of its column. Suppose that the true relationship between regressors $\bmx_1$ and $\bmx_2$ is $\bmx_2=k\bmx_1$. Then we have
	$$\bar{x_2}=k\bar{x_1}\quad \quad \text{ and }\quad \quad s^2_2=k^2s_1^2.$$
	So for collinear columns $\bmx_1$ and $\bmx_2$, the entries of scaled columns $\bmu_1$ and $\bmu_2$ are
	$$u_{i1}=\frac{x_{i1}-\bar{x_1}}{s_1}\quad\quad\text{ and }\quad \quad u_{i2}=\frac{x_{i2}-\bar{x_2}}{s_2}=\frac{kx_{i1}-k\bar{x_1}}{ks_1}=\frac{x_{i1}-\bar{x_1}}{s_1}=u_{i1}.$$
	
	
	
	%%------------------------------------------------
	% bibliography
	\bibliographystyle{plain}
	\bibliography{bayesref}  % reference informations are contained in references.bib 
	
\end{document}